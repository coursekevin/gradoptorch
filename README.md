# gradoptorch
Gradient based optimizers for python with a PyTorch backend. This package intends to allow more fine-grain control of optimizers without getting lost in the high-level API.
